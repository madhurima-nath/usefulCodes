{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Unzip zipped folder in Sagemaker\nIf the data is in a zip folder, and \ncopied into S3, the following steps will\n- load the zip file into Sagemaker instance\n- unzip all files"},{"metadata":{"trusted":false},"cell_type":"code","source":"! aws s3 cp s3://bucketname/filename.zip .","execution_count":1,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"! unzip -o filename.zip -d zip_contents > stdout; echo -n 'files unzipping completed'","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The `stdout` file contains progress of the unzipped files.\nOnce the files are unzipped, the message\n`files unzipping completed` will be printed."},{"metadata":{},"cell_type":"markdown","source":"## Load unzipped files to S3\nThe function `upload_to_s3` will upload files from\nlocal Sagemaker instance to S3 bucket.\n\nNote: This process takes a long time.\nFor a large number of files, some asynchronous method\nhas to be applied. Using `lambda` function\nor `cli` method could also help.\nHave not explored those options yet."},{"metadata":{"trusted":false},"cell_type":"code","source":"# install required packages\nimport boto3\nimport os\nimport time","execution_count":2,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# define s3 bucketname, folderpath for boto3 session\n\ns3 = boto3.resources('s3')\nbucket_name = 'bucketname'\nfolder_path = 'zip_contents'\nbucket = s3.Bucket(bucket_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def upload_to_s3(folder, file):\n    '''\n    function to upload all contents of\n    zipped folder into s3 bucket\n    using put_object method\n    '''\n    key = folder + '/' + file\n    data = open(key, 'rb')\n    bucket.put_object(Key=key, Body=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# walk through zipped folder to upload files into s3\n\nstTime = time.time()\n\nfor root, dirs, files in os.walk(folder_path):\n    for name in files:\n        upload_to_s3(root, name)\n\n# keep track of time taken to complete the process\nendTime = time.time()\ntTime = (endTime - stTime)/3600     #convert to hours\n\nprint(f'Time taken to upload all files into s3 bucket: {tTime:.2f} hours')\nprint('process completed - all files uploaded')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load data from S3 into dataframe\nThe following cells\n- load contents of `.txt` files, filename into a dictionary\n- convert dictionary into dataframe and save as a `.pkl` file locally"},{"metadata":{"trusted":false},"cell_type":"code","source":"# install required packages\nimport boto3\nimport pandas as pd\nimport time","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# define bucketname for boto3 session\n\ns3 = boto3.resource('s3')\nbucket_name = 'bucketname'\nbucket = s3.Bucket(bucket_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# declare empty dict\ndocList = {}\n\n# load all txt files and contents in dict\nstTime = time.time()\n\nfor obj in bucket.objects.all():\n    key = obj.key\n    if key.endswith('.txt'):\n        # extracting only filename part\n        fname = key.split('/path')[-1]\n        \n        # dict key-value pair -> filename-contents\n        docList[fname] = obj.get()['Body'].read().decode('utf-8')\n        \n# keep track of time taken to complete the process\nendTime = time.time()\ntTime = = (endTime - stTime)/3600   #convert to hours\n\nprint(f'Time taken to load all files and contents into dictionary: {tTime:.2f} hours')\nprint('process completed')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# check entries in the dict\n\nlist(docList.items())[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# convert dict into dataframe\n\ndf = pd.DataFrame(docList.items(), columns = ['FileName', 'RawContents'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# save dataframe as a pickle file\n\ndf.to_pickle('docList.pkl')","execution_count":1,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}